\subsection*{Input methods and dimensional reduction}

We compare the classification results of three different inputs: Using
the processed spectra (\textbf{spectra}), after application of principle component analysis
(\textbf{PCA}) and after application of signal dissection by correlation
maximisation (\textbf{SDCM}).

\subsubsection*{Data format}
The measurement data exists as a set of files containing absolute intensity
values in the range of \SI{294}{\nano\meter} to \SI{1002}{\nano\meter}. Each
file was associated a meta-file containing information about the measured
sample, and a background file, containing the background measurement for that
particular batch. The relevant entries in the meta file are
\begin{description}
    \item[Type:] Name of measured sample material. Here all PE classes (HDPE,
    LDPE) are grouped as PE.
    \item[Origin:] Either name of manufacturer or location of discovery or
    purchase
    \item[Color:] Natural color of the sample
    \item[isPlastic:] Boolean value determining whether sample was plastic or
    natural
    \item[SampleID:] Unique ID identifying the sample for each measurement
\end{description}

In the following, $i$ refers to the rows of the 
\subsubsection*{Data preprocessing for classification}
\label{ssub:preprocForClassification}
A flowchart for the data processing pipeline before classification is shown in
\autoref{sfig:dataprep}.

First, the data (including the background measurement) was interpolated onto a
common spectral axis. Here the number of spectral bins was kept equal the to the
mean of number of bins in the overall set. From each measurement the associated background
measurement was subtracted. The data was concatenated into a single \todo{value}matrix. The peak of spectrometer laser is located at
\SI{408}{\nano\meter}\todo{Correct value}. As we don't expect any measurable
excitations below the laser peak, the offset from the zero-line of each
measurement is estimated by taking the median of the data in the spectral range
\SI{294}{\nano\meter} to \SI{400}{\nano\meter}. Similarly, we estimate the noise
level $\eta^i$ of each measurement by calculating the standard deviation in that range.
As we regard any offset of the spectra as systemic, we subtract it from the
data.

To detect the measurements with experimental overexposure, we determined for
each sample the maximum $max_{\text{smoothed}}^i$ of its smoothed spectrum
$s_{\text{movMedian,20}}$,
where the spectrum was smoothed with a running median of windowsize
\SI{20}{\nano\meter}. The exposure level was calulated by
$\epsilon_{\text{exposure}} = \frac{o_m^i}{max_{\text{smoothed}}^i}$.
Measurements with $\epsilon_{\text{exposure}} < 0.5$ were removed from the data.
\todo{how many were removed?}

The power of the spectrum was calculated as $P^i =
\sqrt{\mean(s_{\text{movMedian,20}}^2)}$, where the squaring was performed for
each spectral bin, and the mean is taken across the whole spectrum. The
signal-to-noise-ratio was calculated as $SNR^i = \frac{P^i}{\eta^i}$.
Measurements with $SNR^i < 2$ were removed from the data. \todo{How many meas.
were dropped?}

Only measurements for which the precise originating sample is known were kept in
the data.

The data to be processed was selected in the ranges
\SIrange{410}{680}{\nano\meter} (\emph{short}) and
\SIrange{410}{1002}{\nano\meter} (\emph{long}). \todo{Change this if only one
version is used}

Each measurement was divided by its norm $n^i$ which was calculated via
\todo{Decide on power, integral or max}. This step is especially important for
SDCM, as otherwise the regression steps (see \autoref{sssub:methodsSDCM}) might
not converge, or converge very slowly. \todo{Discuss choice of norm with spectra
pictures}

In the next step, we performed a\todo{or several, if mutliple IV splits are
made} startified split of the data into a
\emph{preTraining}(\SI{80}{\percent}) and \emph{validation}(\SI{20}{\percent})
set. The material type and sample-ID information were used as stratification
variables.

Lastly we calculated the median of each spectral bin in the preTraining set and
subtracted it from each measurement in both the preTraining and validation set.
This centers the data at the zero level of each spectral bin. Note that this
might introduce artifacts in data sets that have a different median (e.g. due to
being sampled differently), and thus descrease prediction accuracy in
\emph{external} validation sets.\todo{Relevant? Alternatives?}

We additionally performed classifications of spectra and PCA without median
subtraction. As this did not significantly influence the results for PCA, only
the spectra variant is kept in the presented results.

\subsubsection*{Data preprocessing for interpretation}
The data preprocessing for the interpretation of signatures followed essentially
the same steps as in the previous subsection. As we want to be
able to interpret the discoverd clusters (signatures), we imposed the additional
restriction that for each measurement, the meta data for type, origin, color,
isPlastic and sampleID must be provided, which reduced the number of
measurements to $1243$. Furthermore, we did not split off any
validation sets.
\subsubsection*{SDCM}
\label{ssub:methodsSDCM}
SDCM (Signal Dissection by Correlation Maximisation) is a non-supervised
algorithm for the detection of superposed correlations in high-dimensional
datasets. Initially developed for the application in bioinformatics for the
dissection of gene expression data, it has since been successfully applied
in other areas such as photoluminescence spectra of microplastic particles,
or \todo{insert other area}.

We denote $\mathbb{M}^{x,y}$ as the set of real valued $x \times y$
matrices, and $n_f$ as the number of features in the data and $n_s$ the
number of measurements.

At its basis, SDCM assumes the data $\mathcal{L} \in \mathbb{M}^{n_f, n_s}$,
exists as a superposition $\mathcal{L} = \sum_{k=1}^n E_k + \eta$ of submatrices
$E_k \in \mathbb{M}^{n_f^k, n_s^k}$ and residual noise $\eta$. We interpret the
$E_k$ as the physical meaningful correlations in the data.

As superpostion is a non-bijective operation, the dissection of $\mathcal{L}$ back
into separate so-called \emph{signatures }$E_k$ is ambigouous and thus
impossible without further assumptions. Therefore we further assume that the
$E_k$ are \emph{bimonotonic}, meaning that there exists an ordering of the
$n_f^k$ indices $I_f$ and an ordering of the $n_s^k$ indices $I_s$ such that
the reordered matrix $\tilde{E}_k = {E(I_f, I_s)}_k$ is montonic along all
rows and columns. While this second assumptions restricts the applicability
of the algorithm, it allows for an unambigous dissection of $\mathcal{L}$ into the
$E_k$ components. Other than comparable methods such as PCA, it also allows
for non-linear montonic correlations.

SDCM dissections the data in four steps:
\begin{enumerate}
    \item First, initial representatives for an axis of correlation are
        detected
    \item The signature axes are calculated by maximising the correlation
    \item A bimontonic regression find the (bimontonic), possibly
        non-linear, \emph{eigensignal} of the signature
    \item The data points belonging to that signature are subtracted from
        the data set.
\end{enumerate}
The algorithm terminates once no more represenatives of axes can be found.
SDCM treats its rows and columns as completely symmetrical. Each feature and
sample can now be given a strength and weight value $s$. The strength value (in
units of the signature) determines how far along the axis (in feature- or
sample space) the point lies. The weight $w\in[-1,1]$ quantifies how
strongly the feature or sample participates in the signature.

Typically, the number of signatures detected will be orders of magnitudes
smaller than the number of input features, thus allowing for an effective
dimensional reduction of the data. 


\subsubsection*{Dimensional reduction}
We used SDCM and PCA to reduce the number of dimensions of the data, and to
obtain signature strenghts and weights in the case of SDCM, and PC axes in
feature space and PC coefficients in the case of PCA. For classification, this
reduction was performed once on each \emph{preTraining} set. For interpretation,
it was performed once on the data. 

To keep the number of principal components to a comparable size with the
signatures in SDCM, and to achieve an actual dimensional reduction, we chose the
first $n$ principal components that account for $\SI{99}{\percent}$ of the
variance \todo{change this if necessary}.

The distribution of number of signatures found by SDCM and principal components
found by PCA is show in \todo{this
probably has to be done }

Once a set of SDCM signatures has been found in the \emph{preTraining} set, obtaining
strengths and weights relative to these signatures for new data, i.e.
\emph{validation} set is a non-trivial task, as the signature axes can be
non-orthogonal, and the method is dissective rather than just a projection. The
standard procedure is to repeat the dissection on the new data, while fixing the
signature axes to the previously detected values. \todo{What exactly is the
problem here? Signature axes should be fixed either way.}

To circumvent this issue, we performed a weighted projection the new data onto
the known signature axes \todo{insert math}. This removes some of the
precision obtained by SDCM, as spectral features explained by a single signature
can still produce significant projection values in other signatures, if the axes
aren't sufficiently orthogonal.
However, it ensures that all signature strengths are obtained relative to the
same axes. For consistency, the \emph{preTraining} set was also projected onto its
own axes. \todo{difference signatures strength, projection values} 

For interpretation purposes this step isn't needed, as we only considered a
single data set without validation splits. Here we used the output signature
strengths and weights.
