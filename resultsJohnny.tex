\subsection*{Performance evaluation of ML models}
\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{figures/metric_evaluation.png}
	\caption{Performance overview of ML models for plastic classification. 
		The black bars represent the standard deviations of the performance metrics.}
	\label{fig:metricComparison}
\end{figure}
The prediction performance of a ML model depends on the intended application, the data transformation algorithm and the learning method.
Therefore, we first compare all ML models that are generated with all possible combinations of transformation algorithm and learning method.
For our evaluation, we calculate the performance metrics \textit{accuracy}, \textit{precision}, \textit{recall} and \textit{f1}.
\autoref{fig:metricComparison} gives an overview of the metrics for all generated models.
We immediately see that the learning method has the biggest influence on the model's performance.
Models generated with the SVC, NuSVC, Logistic Regression and the Random Forest algorithms achieve values over \SI{90}{\percent} for all metrics.
On the other hand, it drops by \textbf{XXX} for models generated with the naive Bayes method.

The influence of the transformation algorithm, on the other hand, depends on the selected learning method.
We find similar performances for models generated with the NuSVC and the logistic regression method, while for the rest, a data dimension reduction improves the performance by at least \SI{2}{\percent}.
Interestingly, we see a trend that linear classifiers, i.e. SVC, NuSVC and logistic regression, work better with SDCM transformed data while non-linear classifiers, i.e. naive Bayesian work best with PCA transformed data.

Our evalutation shows clearly that the prediction performance for most of the ML models is high.
Slight improvements can be achieved by chosing the right combination of data transformation and classifier.
Note, that further improvements may also be possible by optimising the hyperparameters associated with the selected classifier.

\subsection*{Classification performance of ML models}

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{figures/confusionMatrix_validation.png}
	\caption{Confusion matrix of the validation set for individual sample
		classes and classifiers, normalized along rows. The heatmaps are to be read
		as \enquote{in <p>\%
			of all predictions <row> is classified as <column>}. As the Naive Bayes
		model scores significantly worse in all metrics than the other classifiers,
		it has been omitted from the image.}
	\label{fig:confusionMatrix}
\end{figure}

Different combintations of data transformation technique and classifier may work best for certain sample types in our dataset.
\autoref{fig:confusionMatrix} presents a confusion matrix of all ML models generated with metric values over \SI{90}{\percent}.
We see that without any data transformation, i.e. spectra and spetra (no batchprocessing), ML models confuse PVC samples as nonplastic samples whereas with PCA and SDCM this problem is no longer present.
Furthermore, we see similar performances for all ML models that are generated with PCA- and SDCM-transformed data.
